{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0bc42d",
   "metadata": {},
   "source": [
    "# Demostration of Different Predictors and Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0b0f1",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0d34",
   "metadata": {},
   "source": [
    "### Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb430770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 5, 3, 3],\n",
    "    [3, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 5, 4],\n",
    "    [5, 1, 4, 2, 4],\n",
    "    [2, 3, 4, 1, 1],\n",
    "    [2, 3, 4, 2, 5],\n",
    "])\n",
    "\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (0, 3),\n",
    "    (1, 1),\n",
    "    (1, 4),\n",
    "    (2, 0),\n",
    "    (2, 4),\n",
    "    (3, 2),\n",
    "    (4, 1),\n",
    "    (4, 3),\n",
    "    (5, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d794450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [5, 4, 4, 0, 5],\n",
    "        [0, 3, 5, 3, 4],\n",
    "        [5, 2, 0, 2, 3],\n",
    "        [0, 2, 3, 1, 2],\n",
    "        [4, 0, 5, 4, 5],\n",
    "        [5, 3, 0, 3, 5],\n",
    "        [3, 2, 3, 2, 0],\n",
    "        [5, 3, 4, 0, 5],\n",
    "        [4, 2, 5, 4, 0],\n",
    "        [5, 0, 5, 3, 4],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 4),\n",
    "    (1, 3),\n",
    "    (2, 3),\n",
    "    (3, 1),\n",
    "    (4, 2),\n",
    "    (5, 0),\n",
    "    (6, 1),\n",
    "    (7, 1),\n",
    "    (8, 0),\n",
    "    (9, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8cd12",
   "metadata": {},
   "source": [
    "### Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samples import generate_sample_data\n",
    "import json\n",
    "\n",
    "sample_data = generate_sample_data(1000, 1000, 200)\n",
    "with open(\"samples/data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff1a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "with open(\"samples/data.json\", \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "user_ratings = sample_data[\"ratings\"]\n",
    "test_set_size = int(len(user_ratings) * len(user_ratings[0]) * 0.2)\n",
    "test_set = [(randrange(0, len(user_ratings)), randrange(0, len(user_ratings[0]))) for _ in range(test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eecd0c",
   "metadata": {},
   "source": [
    "### Real Datasets (from MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609255c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the latest (small) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")\n",
    "\n",
    "# Download the latest (full) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV files...\n",
      "Pivotting data...\n",
      "User ratings table created with dimensions: 330975 rows x 83239 columns\n",
      "Making test sets...\n",
      "Test set created with size: 6766432\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"Reading CSV files...\")\n",
    "# Read the ratings and movies CSV\n",
    "# ratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\")\n",
    "# movies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings and movies CSV (WARNING: FULL DATASET)\n",
    "ratings_df = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "movies_df = pd.read_csv(\"data/ml-latest/movies.csv\")\n",
    "\n",
    "# Convert the CSV into a user ratings table\n",
    "# Create a dense matrix where each row represents a user and each column a movie.\n",
    "# Missing ratings are filled with 0.\n",
    "print(\"Pivotting data...\")\n",
    "user_ids = sorted(ratings_df[\"userId\"].unique())\n",
    "movie_ids = sorted(ratings_df[\"movieId\"].unique())\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "rows = ratings_df[\"userId\"].map(user_id_map).values\n",
    "cols = ratings_df[\"movieId\"].map(movie_id_map).values\n",
    "data = ratings_df[\"rating\"].values\n",
    "\n",
    "user_ratings = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "movie_id_mappings = movies_df[\"movieId\"].to_list()\n",
    "\n",
    "print(\n",
    "    \"User ratings table created with dimensions:\",\n",
    "    user_ratings.shape[0],\n",
    "    \"rows x\",\n",
    "    user_ratings.shape[1],\n",
    "    \"columns\",\n",
    ")\n",
    "\n",
    "print(\"Making test sets...\")\n",
    "# Get all indices with an existing (non zero) rating\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "shuffuled_valid_entries = sample(valid_entries, k=len(valid_entries))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = shuffuled_valid_entries[:test_set_size]\n",
    "training_set = shuffuled_valid_entries[test_set_size:]\n",
    "print(\"Test set created with size:\", len(test_set))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cbf519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:13<00:00, 503934.02it/s] \n",
      "100%|██████████| 305637/305637 [00:06<00:00, 47508.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:12<00:00, 535814.91it/s] \n",
      "100%|██████████| 305637/305637 [00:08<00:00, 37454.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_test_set_matrix, remove_test_set\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d69ca",
   "metadata": {},
   "source": [
    "## Rating Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3cb7",
   "metadata": {},
   "source": [
    "### Least Squares Optimiation Predictor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa88af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 2659870.09it/s]\n",
      "100%|██████████| 609/609 [00:00<00:00, 35635.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 3867166.31it/s]\n",
      "100%|██████████| 609/609 [00:00<00:00, 30638.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n",
      "Constructing relevant matrices...\n",
      "Calculating user and item biases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 1702867.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80669/80669 [00:00<00:00, 1834682.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([3.1549098 , 2.13517279, 3.91370611, ..., 3.16089698, 2.94281606,\n",
      "       3.80851318], shape=(20167,))\n",
      "training_predictions = array([2.20713821, 4.42210064, 3.24345784, ..., 4.06639076, 2.31312133,\n",
      "       3.88218236], shape=(80669,))\n",
      "training_data.data = array([4., 4., 4., ..., 4., 5., 3.], shape=(80669,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.7784721291176996)\n",
      "rmse_test = np.float64(0.8750824501836783)\n"
     ]
    }
   ],
   "source": [
    "from predictors.least_squares import LeastSquaresPredictor\n",
    "from utils import get_test_set_matrix, remove_test_set, root_mean_square_error, root_mean_square_error_entries\n",
    "\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)\n",
    "baseline = LeastSquaresPredictor(shape=user_ratings.shape, lmda=0.2)\n",
    "baseline.train(training_data=training_data)\n",
    "test_predictions = baseline.predict(test_set)\n",
    "training_predictions = baseline.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f4b1",
   "metadata": {},
   "source": [
    "### Neighbor Correlations Predictor (based on Least Sqaures Optimization) (Improved)\n",
    "As this requires calculating cosine coefficient for every single pair of items (or users, depending on the correlation chosen), this is extremely computationally expensive. Not recommended for data with a large number of items (or users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af6f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cosine similarity coefficients...\n",
      "Making neighbor table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9724/9724 [00:09<00:00, 1035.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 89265.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80669/80669 [00:00<00:00, 93488.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([2.6100543 , 1.53398421, 3.19146798, ..., 3.68492274, 3.60581692,\n",
      "       3.53463773], shape=(20167,))\n",
      "training_predictions = array([2.20713821, 5.        , 5.        , ..., 3.78632189, 2.49114979,\n",
      "       4.06934193], shape=(80669,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from predictors.neighbor_correlations import Correlation, NeighborCorrelationsPredictor\n",
    "from utils.neighbor_selection import most_similar, two_most_similar_skip_masked, two_most_similar\n",
    "\n",
    "# baseline.predict_all = lambda quiet=False: np.array(\n",
    "#     [\n",
    "#         [np.nan, 2.7, 3.3, np.nan, 4.5],\n",
    "#         [4.1, np.nan, 3.5, 4.9, np.nan],\n",
    "#         [np.nan, 3.8, 2.5, 4.2, np.nan],\n",
    "#         [2.8, 3.1, np.nan, 2.6, 4.8],\n",
    "#         [3.3, np.nan, 3.7, np.nan, 2.4],\n",
    "#         [np.nan, 3.9, 4.0, 1.5, 3.9],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n",
    "improved.train(training_data, most_similar)\n",
    "test_predictions = improved.predict(test_set)\n",
    "training_predictions = improved.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad0bce",
   "metadata": {},
   "source": [
    "### Latent Factor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [3, 4, 5, 3, 2, 3],\n",
    "        [3, 2, 3, 4, 2, 1],\n",
    "        [4, 4, 4, 5, 3, 2],\n",
    "        [3, 5, 4, 4, 3, 4],\n",
    "        [2, 1, 2, 2, 3, 1],\n",
    "        [3, 5, 5, 4, 4, 3],\n",
    "        [3, 5, 5, 3, 2, 2],\n",
    "        [2, 3, 3, 2, 1, 2],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 0),\n",
    "    (3, 1),\n",
    "    (5, 1),\n",
    "    (5, 4),\n",
    "    (6, 0),\n",
    "    (6, 2),\n",
    "    (7, 1),\n",
    "    (7, 3),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd8b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent.p = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 330975))\n",
      "latent.q = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 83239))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.latent_factor import LatentFactorPredictor\n",
    "\n",
    "u, i = training_data.shape\n",
    "k = 2\n",
    "latent = LatentFactorPredictor(\n",
    "    shape=training_data.shape,\n",
    "    k=k,\n",
    "    p=np.ones(shape=(k,u), dtype=np.float64),\n",
    "    q=np.ones(shape=(k,i), dtype=np.float64),\n",
    "    lmda=0.2,\n",
    ")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b832b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Performing alternating least squares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:42<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent.train(training_data=training_data, iterations=100)\n",
    "# t += 20\n",
    "# print(f\"Total: {t} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d91660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27065730/27065730 [05:30<00:00, 81902.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [01:21<00:00, 83480.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "training_predictions = array([3.96516049, 1.07460525, 1.51735067, ..., 2.37272822, 3.72034409,\n",
      "       4.55430946], shape=(27065730,))\n",
      "test_predictions = array([2.93717671, 3.78370921, 2.32394171, ..., 2.8064506 , 3.87181031,\n",
      "       3.53199553], shape=(6766432,))\n",
      "latent.p = array([[1.23401304, 1.01703323, 1.31431579, ..., 1.07690985, 0.80702365,\n",
      "        0.46423131],\n",
      "       [0.90829139, 0.97683121, 1.12561913, ..., 0.89869366, 0.97421212,\n",
      "        0.77556854]], shape=(2, 330975))\n",
      "latent.q = array([[1.54200499, 2.90340254, 2.99129576, ..., 1.72557883, 1.46030548,\n",
      "        1.45280941],\n",
      "       [2.50311193, 0.44089136, 0.14449253, ..., 1.91721313, 1.2657258 ,\n",
      "        2.02169956]], shape=(2, 83239))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_predictions = latent.predict(training_set)\n",
    "test_predictions = latent.predict(test_set)\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9695964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.8073885656987553)\n",
      "rmse_test = np.float64(0.8371278613256798)\n"
     ]
    }
   ],
   "source": [
    "from utils import root_mean_square_error_entries, root_mean_square_error\n",
    "\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e5ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/latent', 'wb') as f:\n",
    "    pickle.dump(latent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f670e",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a49a7",
   "metadata": {},
   "source": [
    "### Plain Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1445f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.plain import PlainRecommender\n",
    "\n",
    "recommender = PlainRecommender(\n",
    "    predictor=latent, users=user_ratings.shape[0], items=user_ratings.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc4f74",
   "metadata": {},
   "source": [
    "### Pure Score Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9260f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bids: [(55286, 0.9902403880506729), (54738, 0.9772386033079312), (64276, 0.9651193228294344), (3869, 0.947772013374295), (42794, 0.8986451166915099), (28595, 0.8941967902781269), (80144, 0.8873161444495223), (40783, 0.8747367723727055), (54554, 0.8731391878636571), (65552, 0.8685857450545172), (45586, 0.8535867166808995), (64479, 0.8516122509142184), (70450, 0.8505061069245037), (47754, 0.8293073384094706), (16924, 0.825801620356268), (46910, 0.8083320033082336), (78917, 0.8067832103131113), (1474, 0.804966835097827), (47686, 0.7959335915196324), (3767, 0.7862560632745693), (29478, 0.7627977849820808), (80449, 0.7494890094926858), (81921, 0.7311121142057868), (67775, 0.6868019702258791), (41341, 0.6801606656452964), (4174, 0.6639739807788596), (46939, 0.6589829717782755), (51980, 0.6462623107839929), (53783, 0.5875176115718594), (18528, 0.5862603713234524), (36710, 0.5839041896510366), (76385, 0.5366528932021754), (37402, 0.5172234829056793), (27860, 0.5120540635978971), (50837, 0.46151000075999904), (73611, 0.3293997225969111), (65897, 0.3274898740396953), (50187, 0.29881129734889345), (31499, 0.28606944004958423), (61841, 0.25192269910980647), (46748, 0.24954833036864477), (68632, 0.2395072028484342), (37339, 0.22354768637783107), (83116, 0.2232339475198537), (16504, 0.21116104367819077), (50930, 0.19010518534019782), (67399, 0.16645374816665603), (32309, 0.11595109594001374), (20387, 0.10741779450334821), (66651, 0.01427431632062437)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from recommenders.score_boost import ScoreBoostRecommender\n",
    "\n",
    "bids = [\n",
    "    (idx, random.random()) for idx in random.sample(range(user_ratings.shape[1]), k=50)\n",
    "]\n",
    "paid_recommender = ScoreBoostRecommender(\n",
    "    predictor=latent,\n",
    "    users=user_ratings.shape[0],\n",
    "    items=user_ratings.shape[1],\n",
    "    bids=bids,\n",
    "    alpha=0.1,\n",
    "    beta=50,\n",
    "    promotion_slots=[True if x % 4 == 0 else False for x in range(20)]\n",
    ")\n",
    "print(\"Bids:\", sorted(bids, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02397b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without promotion: 158336 [236125, 207796, 155637, 276235, 179189, 207369, 155399, 162514, 178403, 192287, 276123, 276097, 213644, 238032, 66904, 271739, 103741, 103769, 154592, 87700]\n",
      "With promotion: 158336 [188597, 215003, 214142, 215001, 220766, 205663, 249232, 266080, 151773, 214342, 214260, 214174, 210173, 213644, 189143, 162514, 172303, 236125, 277580, 131777]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# print(recommender.users, recommender.items)\n",
    "# user = randint(0, user_ratings.shape[0])\n",
    "user = 158336\n",
    "print(\"Without promotion:\", user, [movie_id_mappings[x] for x in recommender.recommend_items(user, 20)])\n",
    "print(\"With promotion:\", user, [movie_id_mappings[x] for x in paid_recommender.recommend_items(user, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba902e",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
