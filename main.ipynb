{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0bc42d",
   "metadata": {},
   "source": [
    "# Demostration of Different Predictors and Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0b0f1",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0d34",
   "metadata": {},
   "source": [
    "### Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb430770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 5, 3, 3],\n",
    "    [3, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 5, 4],\n",
    "    [5, 1, 4, 2, 4],\n",
    "    [2, 3, 4, 1, 1],\n",
    "    [2, 3, 4, 2, 5],\n",
    "])\n",
    "\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (0, 3),\n",
    "    (1, 1),\n",
    "    (1, 4),\n",
    "    (2, 0),\n",
    "    (2, 4),\n",
    "    (3, 2),\n",
    "    (4, 1),\n",
    "    (4, 3),\n",
    "    (5, 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d794450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 4, 0, 5],\n",
    "    [0, 3, 5, 3, 4],\n",
    "    [5, 2, 0, 2, 3],\n",
    "    [0, 2, 3, 1, 2],\n",
    "    [4, 0, 5, 4, 5],\n",
    "    [5, 3, 0, 3, 5],\n",
    "    [3, 2, 3, 2, 0],\n",
    "    [5, 3, 4, 0, 5],\n",
    "    [4, 2, 5, 4, 0],\n",
    "    [5, 0, 5, 3, 4]\n",
    "])\n",
    "test_set = [(0, 4), (1, 3), (2, 3), (3, 1), (4, 2),\n",
    "                   (5, 0), (6, 1), (7, 1), (8, 0), (9, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8cd12",
   "metadata": {},
   "source": [
    "### Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samples import generate_sample_data\n",
    "import json\n",
    "\n",
    "sample_data = generate_sample_data(1000, 1000, 200)\n",
    "with open(\"samples/data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff1a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "with open(\"samples/data.json\", \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "user_ratings = sample_data[\"ratings\"]\n",
    "test_set_size = int(len(user_ratings) * len(user_ratings[0]) * 0.2)\n",
    "test_set = [(randrange(0, len(user_ratings)), randrange(0, len(user_ratings[0]))) for _ in range(test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eecd0c",
   "metadata": {},
   "source": [
    "### Real Datasets (from MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609255c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the latest (small) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")\n",
    "\n",
    "# Download the latest (full) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivotting data...\n",
      "User ratings table created with dimensions: 610 rows x 9724 columns\n",
      "Making test sets...\n",
      "Test set created with size: 20167\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Read the ratings and movies CSV\n",
    "ratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\")\n",
    "movies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings and movies CSV (WARNING: FULL DATASET)\n",
    "# print(\"Reading CSV files...\")\n",
    "# ratings_df = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "# movies_df = pd.read_csv(\"data/ml-latest/movies.csv\")\n",
    "\n",
    "# Convert the CSV into a user ratings table\n",
    "# Create a dense matrix where each row represents a user and each column a movie.\n",
    "# Missing ratings are filled with 0.\n",
    "print(\"Pivotting data...\")\n",
    "user_ids = sorted(ratings_df[\"userId\"].unique())\n",
    "movie_ids = sorted(ratings_df[\"movieId\"].unique())\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "rows = ratings_df[\"userId\"].map(user_id_map).values\n",
    "cols = ratings_df[\"movieId\"].map(movie_id_map).values\n",
    "data = ratings_df[\"rating\"].values\n",
    "\n",
    "user_ratings = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "movie_id_mappings = movies_df[\"movieId\"].to_list()\n",
    "\n",
    "print(\n",
    "    \"User ratings table created with dimensions:\",\n",
    "    user_ratings.shape[0],\n",
    "    \"rows x\",\n",
    "    user_ratings.shape[1],\n",
    "    \"columns\",\n",
    ")\n",
    "\n",
    "print(\"Making test sets...\")\n",
    "# Get all indices with an existing (non zero) rating\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = sample(valid_entries, min(test_set_size, len(valid_entries)))\n",
    "print(\"Test set created with size:\", len(test_set))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d69ca",
   "metadata": {},
   "source": [
    "## Rating Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3cb7",
   "metadata": {},
   "source": [
    "### Least Squares Optimiation Predictor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa88af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing relevant matrices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating user and item biases...\n",
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 153637.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting all...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 655360.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting all.\n",
      "test_predictions = array([4.61784512, 3.48611111, 2.78114478, 1.        , 4.78998316,\n",
      "       4.87710438, 1.22895623, 2.91919192, 4.78451178, 4.60984848])\n",
      "all_predictions.data = array([5.        , 3.09385522, 4.8956229 , 2.89141414, 4.69318182,\n",
      "       4.41540404, 4.10311448, 2.18644781, 3.71043771, 2.49494949,\n",
      "       1.28787879, 2.21717172, 4.90488215, 3.58291246, 4.51220539,\n",
      "       2.96043771, 3.55513468, 4.48442761, 3.1456229 , 3.03072391,\n",
      "       1.8236532 , 4.83585859, 4.7209596 , 4.44318182, 2.86784512,\n",
      "       4.66961279, 3.46254209, 4.49494949, 3.28787879, 4.21717172])\n",
      "training_data.data = array([5, 4, 4, 3, 5, 4, 5, 2, 3, 3, 1, 2, 4, 4, 5, 3, 3, 5, 3, 3, 2, 5,\n",
      "       4, 5, 2, 5, 4, 5, 3, 4])\n",
      "rmse_training = np.float64(0.3970719531355135)\n",
      "rmse_test = np.float64(0.5848469310964864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from predictors.least_squares import LeastSquaresPredictor\n",
    "from utils import get_test_set_matrix, remove_test_set, root_mean_square_error, root_mean_square_error_entries\n",
    "\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)\n",
    "baseline = LeastSquaresPredictor(training_data=training_data, lmda=0)\n",
    "baseline.train()\n",
    "test_predictions = baseline.predict(test_set)\n",
    "all_predictions = baseline.predict_all()\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{all_predictions.data = }\")\n",
    "print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error(all_predictions, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f4b1",
   "metadata": {},
   "source": [
    "### Neighbor Correlations Predictor (based on Least Sqaures Optimization) (Improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af6f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cosine similarity coefficients...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# baseline.predict_all = lambda quiet=False: np.array(\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     [\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#         [np.nan, 2.7, 3.3, np.nan, 4.5],\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     16\u001b[39m improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mimproved\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwo_most_similar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m test_predictions = improved.predict(test_set)\n\u001b[32m     19\u001b[39m all_predictions = improved.predict_all()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/predictors/neighbor_correlations.py:196\u001b[39m, in \u001b[36mNeighborCorrelationsPredictor.train\u001b[39m\u001b[34m(self, get_neighbors)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, get_neighbors: Any = most_similar):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating cosine similarity coefficients...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28mself\u001b[39m.error = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_less\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m - \u001b[38;5;28mself\u001b[39m.baseline.predict_all(quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.cosine_coefficients = \u001b[38;5;28mself\u001b[39m._find_cosine_coefficients(\u001b[38;5;28mself\u001b[39m.error)\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMaking neighbor table...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/numpy/ma/core.py:2083\u001b[39m, in \u001b[36mmasked_less\u001b[39m\u001b[34m(x, value, copy)\u001b[39m\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmasked_less\u001b[39m(x, value, copy=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   2060\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2061\u001b[39m \u001b[33;03m    Mask an array where less than a given value.\u001b[39;00m\n\u001b[32m   2062\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2081\u001b[39m \n\u001b[32m   2082\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m masked_where(\u001b[43mless\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m, x, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/numpy/ma/core.py:1065\u001b[39m, in \u001b[36m_MaskedBinaryOperation.__call__\u001b[39m\u001b[34m(self, a, b, *args, **kwargs)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate():\n\u001b[32m   1064\u001b[39m     np.seterr(divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# Get the mask for the result\u001b[39;00m\n\u001b[32m   1067\u001b[39m (ma, mb) = (getmask(a), getmask(b))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py:417\u001b[39m, in \u001b[36m_spbase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nnz != \u001b[32m0\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe truth value of an array with more than one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.neighbor_correlations import Correlation, NeighborCorrelationsPredictor\n",
    "from utils.neighbor_selection import most_similar, two_most_similar_skip_masked, two_most_similar\n",
    "\n",
    "# baseline.predict_all = lambda quiet=False: np.array(\n",
    "#     [\n",
    "#         [np.nan, 2.7, 3.3, np.nan, 4.5],\n",
    "#         [4.1, np.nan, 3.5, 4.9, np.nan],\n",
    "#         [np.nan, 3.8, 2.5, 4.2, np.nan],\n",
    "#         [2.8, 3.1, np.nan, 2.6, 4.8],\n",
    "#         [3.3, np.nan, 3.7, np.nan, 2.4],\n",
    "#         [np.nan, 3.9, 4.0, 1.5, 3.9],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n",
    "improved.train(two_most_similar)\n",
    "test_predictions = improved.predict(test_set)\n",
    "all_predictions = improved.predict_all()\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{all_predictions = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad0bce",
   "metadata": {},
   "source": [
    "### Latent Factor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = [\n",
    "    [3, 4, 5, 3, 2, 3],\n",
    "    [3, 2, 3, 4, 2, 1],\n",
    "    [4, 4, 4, 5, 3, 2],\n",
    "    [3, 5, 4, 4, 3, 4],\n",
    "    [2, 1, 2, 2, 3, 1],\n",
    "    [3, 5, 5, 4, 4, 3],\n",
    "    [3, 5, 5, 3, 2, 2],\n",
    "    [2, 3, 3, 2, 1, 2],\n",
    "]\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 0),\n",
    "    (3, 1),\n",
    "    (5, 1),\n",
    "    (5, 4),\n",
    "    (6, 0),\n",
    "    (6, 2),\n",
    "    (7, 1),\n",
    "    (7, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd8b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent.p = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 610))\n",
      "latent.q = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 9724))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.latent_factor import LatentFactorPredictor\n",
    "from utils import get_test_set_matrix, remove_test_set\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)\n",
    "u, i = training_data.shape\n",
    "k = 2\n",
    "latent = LatentFactorPredictor(\n",
    "    training_data=training_data,\n",
    "    k=k,\n",
    "    p=np.ones(shape=(k,u), dtype=np.float64),\n",
    "    q=np.ones(shape=(k,i), dtype=np.float64),\n",
    "    lmda=0.2,\n",
    ")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b832b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Performing alternating least squares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent.train(100)\n",
    "# t += 20\n",
    "# print(f\"Total: {t} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d91660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 124545.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([4.07793265, 2.55603855, 2.40243762, ..., 2.77691777, 3.50032545,\n",
      "       4.57620272], shape=(20167,))\n",
      "latent.p = array([[ 1.62403868,  1.83073924, -4.85863659, ...,  1.1920012 ,\n",
      "         1.16988134,  0.14669571],\n",
      "       [ 2.66737572,  1.76125342,  4.56756953, ...,  2.05877798,\n",
      "         1.99417846,  3.338372  ]], shape=(2, 610))\n",
      "latent.q = array([[ 1.04391677,  0.98010346,  0.83856835, ...,  0.30166179,\n",
      "         0.30166179, -0.24922798],\n",
      "       [ 1.19333163,  1.03091986,  0.99673874, ...,  1.13461325,\n",
      "         1.13461325,  0.92082421]], shape=(2, 9724))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# all_predictions = latent.predict_all()\n",
    "test_predictions = latent.predict(test_set)\n",
    "# print(f\"{all_predictions = }\")\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9695964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test = np.float64(0.936999622670139)\n"
     ]
    }
   ],
   "source": [
    "from utils import root_mean_square_error_entries, root_mean_square_error\n",
    "\n",
    "# rmse_training = root_mean_square_error(all_predictions, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "# print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f670e",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a49a7",
   "metadata": {},
   "source": [
    "### Plain Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1445f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.plain import PlainRecommender\n",
    "\n",
    "recommender = PlainRecommender(\n",
    "    predictor=latent, users=len(user_ratings), items=len(user_ratings[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc4f74",
   "metadata": {},
   "source": [
    "### Pure Score Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9260f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bids: [(7453, 0.9589942423383562), (7332, 0.9536443366636993), (8700, 0.9489936905621729), (9248, 0.9397598989113214), (6302, 0.9068013002138909), (8436, 0.846906227825697), (2173, 0.8406623230266436), (1982, 0.8340563027496675), (1280, 0.8333641553038239), (1146, 0.8098692294726698), (2051, 0.8023050293707962), (7933, 0.781652244287606), (6719, 0.7415775243675684), (3773, 0.7402966517616344), (3800, 0.7199998517457488), (4164, 0.7121445257941096), (6918, 0.6308846214258115), (9120, 0.6225530927666422), (8139, 0.6132826052726236), (2774, 0.6067683021854938), (1473, 0.6039860630527522), (9001, 0.5883723503072925), (4188, 0.5732438926473697), (5501, 0.5509626691825118), (7510, 0.5501842325241326), (1601, 0.5131548547013771), (3207, 0.4720190037075762), (5275, 0.4595292982980288), (2571, 0.4360051641613203), (4163, 0.4159653177675847), (8110, 0.4064911143371722), (4341, 0.3683120391126532), (5872, 0.32961915273822406), (30, 0.3188970688401671), (1989, 0.2835109754433609), (7002, 0.24067035331869047), (9269, 0.21600811927539776), (3256, 0.19859074476713956), (2744, 0.19632138292598567), (3751, 0.1446929596637362), (4015, 0.12331312962663477), (8615, 0.11690623107429965), (7747, 0.11531347240120249), (9122, 0.09322457911680015), (4336, 0.09174123663060763), (6009, 0.08320262907022868), (815, 0.059981312166553225), (1586, 0.036653627585048976), (4303, 0.008559504883584701), (548, 0.004276406058223103)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from recommenders.score_boost import ScoreBoostRecommender\n",
    "\n",
    "bids = [\n",
    "    (idx, random.random()) for idx in random.sample(range(len(user_ratings[0])), k=50)\n",
    "]\n",
    "paid_recommender = ScoreBoostRecommender(\n",
    "    predictor=latent,\n",
    "    users=len(user_ratings),\n",
    "    items=len(user_ratings[0]),\n",
    "    bids=bids,\n",
    "    alpha=0.1,\n",
    "    beta=50,\n",
    "    promotion_slots=[True if x % 4 == 0 else False for x in range(20)]\n",
    ")\n",
    "print(\"Bids:\", sorted(bids, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02397b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.19136317 2.38667721 2.38502876 2.29403273 2.18955267 2.15025637\n",
      " 2.08836099 2.05003535 1.96335046 1.94384418 1.93884868 1.90443902\n",
      " 1.90443902 1.89399796 1.88095726 1.87526962 1.85883522 1.85883522\n",
      " 1.84525491 1.84233014]\n",
      "Without promotion: 441 [np.int64(5022), np.int64(9302), np.int64(4392), np.int64(1947), np.int64(374), np.int64(2800), np.int64(4770), np.int64(6218), np.int64(3106), np.int64(2374), np.int64(7242), np.int64(8136), np.int64(7794), np.int64(5641), np.int64(1028), np.int64(6601), np.int64(2304), np.int64(1508), np.int64(9314), np.int64(5445)]\n",
      "With promotion: 441 [np.int64(2051), np.int64(5022), np.int64(9302), np.int64(4392), np.int64(4188), np.int64(1947), np.int64(374), np.int64(2800), np.int64(2173), np.int64(4770), np.int64(6218), np.int64(3106), np.int64(6302), np.int64(2374), np.int64(7242), np.int64(7794), np.int64(1982), np.int64(8136), np.int64(5641), np.int64(1028)]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# print(recommender.users, recommender.items)\n",
    "user = randint(0, len(user_ratings))\n",
    "# user = 261\n",
    "print(\"Without promotion:\", user, [x for x in recommender.recommend_items(user, 20)])\n",
    "print(\"With promotion:\", user, [x for x in paid_recommender.recommend_items(user, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba902e",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
