{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0bc42d",
   "metadata": {},
   "source": [
    "# Demostration of Different Predictors and Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0b0f1",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0d34",
   "metadata": {},
   "source": [
    "### Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb430770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 5, 3, 3],\n",
    "    [3, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 5, 4],\n",
    "    [5, 1, 4, 2, 4],\n",
    "    [2, 3, 4, 1, 1],\n",
    "    [2, 3, 4, 2, 5],\n",
    "])\n",
    "\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (0, 3),\n",
    "    (1, 1),\n",
    "    (1, 4),\n",
    "    (2, 0),\n",
    "    (2, 4),\n",
    "    (3, 2),\n",
    "    (4, 1),\n",
    "    (4, 3),\n",
    "    (5, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d794450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [5, 4, 4, 0, 5],\n",
    "        [0, 3, 5, 3, 4],\n",
    "        [5, 2, 0, 2, 3],\n",
    "        [0, 2, 3, 1, 2],\n",
    "        [4, 0, 5, 4, 5],\n",
    "        [5, 3, 0, 3, 5],\n",
    "        [3, 2, 3, 2, 0],\n",
    "        [5, 3, 4, 0, 5],\n",
    "        [4, 2, 5, 4, 0],\n",
    "        [5, 0, 5, 3, 4],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 4),\n",
    "    (1, 3),\n",
    "    (2, 3),\n",
    "    (3, 1),\n",
    "    (4, 2),\n",
    "    (5, 0),\n",
    "    (6, 1),\n",
    "    (7, 1),\n",
    "    (8, 0),\n",
    "    (9, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8cd12",
   "metadata": {},
   "source": [
    "### Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samples import generate_sample_data\n",
    "import json\n",
    "\n",
    "sample_data = generate_sample_data(1000, 1000, 200)\n",
    "with open(\"samples/data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff1a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "with open(\"samples/data.json\", \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "user_ratings = sample_data[\"ratings\"]\n",
    "test_set_size = int(len(user_ratings) * len(user_ratings[0]) * 0.2)\n",
    "test_set = [(randrange(0, len(user_ratings)), randrange(0, len(user_ratings[0]))) for _ in range(test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eecd0c",
   "metadata": {},
   "source": [
    "### Real Datasets (from MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609255c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the latest (small) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")\n",
    "\n",
    "# Download the latest (full) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV files...\n",
      "Pivotting data...\n",
      "User ratings table created with dimensions: 330975 rows x 83239 columns\n",
      "Making test sets...\n",
      "Test set created with size: 6766432\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"Reading CSV files...\")\n",
    "# Read the ratings and movies CSV\n",
    "# ratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\")\n",
    "# movies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings and movies CSV (WARNING: FULL DATASET)\n",
    "ratings_df = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "movies_df = pd.read_csv(\"data/ml-latest/movies.csv\")\n",
    "\n",
    "# Convert the CSV into a user ratings table\n",
    "# Create a dense matrix where each row represents a user and each column a movie.\n",
    "# Missing ratings are filled with 0.\n",
    "print(\"Pivotting data...\")\n",
    "user_ids = sorted(ratings_df[\"userId\"].unique())\n",
    "movie_ids = sorted(ratings_df[\"movieId\"].unique())\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "rows = ratings_df[\"userId\"].map(user_id_map).values\n",
    "cols = ratings_df[\"movieId\"].map(movie_id_map).values\n",
    "data = ratings_df[\"rating\"].values\n",
    "\n",
    "user_ratings = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "movie_id_mappings = movies_df[\"movieId\"].to_list()\n",
    "\n",
    "print(\n",
    "    \"User ratings table created with dimensions:\",\n",
    "    user_ratings.shape[0],\n",
    "    \"rows x\",\n",
    "    user_ratings.shape[1],\n",
    "    \"columns\",\n",
    ")\n",
    "\n",
    "print(\"Making test sets...\")\n",
    "# Get all indices with an existing (non zero) rating\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "shuffuled_valid_entries = sample(valid_entries, k=len(valid_entries))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = shuffuled_valid_entries[:test_set_size]\n",
    "training_set = shuffuled_valid_entries[test_set_size:]\n",
    "print(\"Test set created with size:\", len(test_set))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cbf519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [01:36<00:00, 69822.18it/s] \n",
      "100%|██████████| 305844/305844 [00:07<00:00, 40596.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [01:10<00:00, 95964.07it/s] \n",
      "100%|██████████| 305844/305844 [00:09<00:00, 32264.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_test_set_matrix, remove_test_set\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d69ca",
   "metadata": {},
   "source": [
    "## Rating Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3cb7",
   "metadata": {},
   "source": [
    "### Least Squares Optimiation Predictor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa88af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing relevant matrices for iterative training...\n",
      "Calculating user and item biases using Conjugate Gradient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1130/10000 [01:10<09:16, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:28<00:00, 237645.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27065730/27065730 [03:07<00:00, 143967.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Converting predictions to csr_matrix...\n",
      "Calculating errors...\n",
      "Calculating sum of square of errors...\n",
      "Converting predictions to csr_matrix...\n",
      "Calculating errors...\n",
      "Calculating sum of square of errors...\n",
      "rmse_training = np.float64(0.8476326427520402)\n",
      "rmse_test = np.float64(0.8623229504665875)\n"
     ]
    }
   ],
   "source": [
    "from predictors.least_squares import LeastSquaresPredictor\n",
    "from utils import root_mean_square_error_entries\n",
    "\n",
    "\n",
    "baseline = LeastSquaresPredictor(shape=user_ratings.shape, lmda=0.2)\n",
    "baseline.iterative_train(training_data=training_data)\n",
    "test_predictions = baseline.predict(test_set)\n",
    "training_predictions = baseline.predict(training_set)\n",
    "# print(f\"{test_predictions = }\")\n",
    "# print(f\"{training_predictions = }\")\n",
    "# print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef31880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing relevant matrices for iterative training...\n",
      "Calculating user and item biases using Conjugate Gradient...\n",
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 2033378.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80669/80669 [00:00<00:00, 1995225.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([3.23165064, 2.24782501, 4.16818974, ..., 3.83512761, 3.45890829,\n",
      "       3.48633018], shape=(20167,))\n",
      "training_predictions = array([3.17870269, 3.55298002, 3.47077656, ..., 3.20558118, 2.99453068,\n",
      "       4.42682523], shape=(80669,))\n",
      "training_data.data = array([4., 4., 4., ..., 5., 5., 3.], shape=(80669,))\n",
      "Converting predictions to csr_matrix...\n",
      "Calculating errors...\n",
      "Calculating sum of square of errors...\n",
      "Converting predictions to csr_matrix...\n",
      "Calculating errors...\n",
      "Calculating sum of square of errors...\n",
      "rmse_training = np.float64(0.7776322133908681)\n",
      "rmse_test = np.float64(0.880068881467453)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline.train(training_data=training_data)\n",
    "test_predictions = baseline.predict(test_set)\n",
    "training_predictions = baseline.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f4b1",
   "metadata": {},
   "source": [
    "### Neighbor Correlations Predictor (based on Least Sqaures Optimization) (Improved)\n",
    "As this requires calculating cosine coefficient for every single pair of items (or users, depending on the correlation chosen), this is extremely computationally expensive. Not recommended for data with a large number of items (or users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af6f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cosine similarity coefficients...\n",
      "Making neighbor table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9724/9724 [00:09<00:00, 1035.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20167/20167 [00:00<00:00, 89265.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80669/80669 [00:00<00:00, 93488.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([2.6100543 , 1.53398421, 3.19146798, ..., 3.68492274, 3.60581692,\n",
      "       3.53463773], shape=(20167,))\n",
      "training_predictions = array([2.20713821, 5.        , 5.        , ..., 3.78632189, 2.49114979,\n",
      "       4.06934193], shape=(80669,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from predictors.neighbor_correlations import Correlation, NeighborCorrelationsPredictor\n",
    "from utils.neighbor_selection import most_similar, two_most_similar_skip_masked, two_most_similar\n",
    "\n",
    "# baseline.predict_all = lambda quiet=False: np.array(\n",
    "#     [\n",
    "#         [np.nan, 2.7, 3.3, np.nan, 4.5],\n",
    "#         [4.1, np.nan, 3.5, 4.9, np.nan],\n",
    "#         [np.nan, 3.8, 2.5, 4.2, np.nan],\n",
    "#         [2.8, 3.1, np.nan, 2.6, 4.8],\n",
    "#         [3.3, np.nan, 3.7, np.nan, 2.4],\n",
    "#         [np.nan, 3.9, 4.0, 1.5, 3.9],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n",
    "improved.train(training_data, most_similar)\n",
    "test_predictions = improved.predict(test_set)\n",
    "training_predictions = improved.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad0bce",
   "metadata": {},
   "source": [
    "### Latent Factor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [3, 4, 5, 3, 2, 3],\n",
    "        [3, 2, 3, 4, 2, 1],\n",
    "        [4, 4, 4, 5, 3, 2],\n",
    "        [3, 5, 4, 4, 3, 4],\n",
    "        [2, 1, 2, 2, 3, 1],\n",
    "        [3, 5, 5, 4, 4, 3],\n",
    "        [3, 5, 5, 3, 2, 2],\n",
    "        [2, 3, 3, 2, 1, 2],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 0),\n",
    "    (3, 1),\n",
    "    (5, 1),\n",
    "    (5, 4),\n",
    "    (6, 0),\n",
    "    (6, 2),\n",
    "    (7, 1),\n",
    "    (7, 3),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd8b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent.p = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(5, 330975))\n",
      "latent.q = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(5, 83239))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.latent_factor import LatentFactorPredictor\n",
    "\n",
    "u, i = training_data.shape\n",
    "k = 5\n",
    "latent = LatentFactorPredictor(\n",
    "    shape=training_data.shape,\n",
    "    k=k,\n",
    "    p=np.ones(shape=(k,u), dtype=np.float64),\n",
    "    q=np.ones(shape=(k,i), dtype=np.float64),\n",
    "    lmda=0.2,\n",
    ")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b832b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Performing alternating least squares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3681/10000 [5:20:56<9:10:56,  5.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent.train(training_data=training_data, max_iterations=10000, tol=1e-4)\n",
    "# t += 20\n",
    "# print(f\"Total: {t} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d91660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27065730/27065730 [06:09<00:00, 73342.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [01:17<00:00, 87283.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "training_predictions = array([4.03971698, 4.01497427, 3.69831421, ..., 2.98674756, 4.31886696,\n",
      "       4.63071298], shape=(27065730,))\n",
      "test_predictions = array([3.86332547, 2.69715931, 3.00076724, ..., 3.92449487, 2.50743115,\n",
      "       4.86522182], shape=(6766432,))\n",
      "latent.p = array([[-0.07819721, -0.19603844,  0.41203395, ...,  0.05589664,\n",
      "        -0.19264802, -0.03581926],\n",
      "       [ 0.51874046,  0.32971636,  0.54947275, ...,  0.64599256,\n",
      "         0.42612923,  0.42105711],\n",
      "       [ 0.2079426 ,  0.60165348,  0.53124531, ...,  0.50344707,\n",
      "         0.88613082,  0.81888573],\n",
      "       [ 0.9978182 ,  0.56970499,  1.21541057, ...,  0.99389135,\n",
      "         0.40230275,  0.36734866],\n",
      "       [ 0.54536394,  0.84827681,  0.71448997, ...,  0.06810958,\n",
      "         0.34297778,  0.10050336]], shape=(5, 330975))\n",
      "latent.q = array([[-1.36015647, -0.30197972, -0.22320699, ...,  0.73504535,\n",
      "         0.41572854,  0.78204922],\n",
      "       [-0.27344002,  0.4465125 ,  0.48633358, ...,  0.61678923,\n",
      "         0.53962818,  0.40939262],\n",
      "       [ 1.83040419,  0.36639761,  0.09892334, ...,  1.72973493,\n",
      "         1.03451235,  1.18521626],\n",
      "       [ 2.05473449,  2.59502267,  2.58239443, ...,  1.37456804,\n",
      "         1.80184782,  0.48788903],\n",
      "       [ 3.24814858,  1.89307303,  1.75915827, ...,  1.2384217 ,\n",
      "         0.35193751,  1.37961376]], shape=(5, 83239))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_predictions = latent.predict(training_set)\n",
    "test_predictions = latent.predict(test_set)\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9695964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6766432 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [01:00<00:00, 111047.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test = np.float64(0.8169738957584134)\n",
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27065730/27065730 [04:37<00:00, 97430.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.7484386486325613)\n"
     ]
    }
   ],
   "source": [
    "from utils import root_mean_square_error_entries, root_mean_square_error\n",
    "\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_test = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "print(f\"{rmse_training = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e5ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/latent_' + str(k), 'wb') as f:\n",
    "    pickle.dump(latent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f670e",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd1b2f",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03c5f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/latent_5', 'rb') as f:\n",
    "    latent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a49a7",
   "metadata": {},
   "source": [
    "### Plain Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1445f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.plain import PlainRecommender\n",
    "\n",
    "recommender = PlainRecommender(\n",
    "    predictor=latent, users=user_ratings.shape[0], items=user_ratings.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc4f74",
   "metadata": {},
   "source": [
    "### Pure Score Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9260f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bids: [(46942, 0.9775303441841998), (21347, 0.9727706273992698), (71436, 0.9701877911829461), (61070, 0.9405803813160011), (74669, 0.9283088788729467), (50598, 0.9241025441836925), (73675, 0.9097545780350953), (17335, 0.9048744484425411), (14248, 0.8999265667689224), (31657, 0.880436228722251), (52392, 0.8762451549359666), (47340, 0.821007160082798), (67945, 0.7586887266949272), (46418, 0.7360430112066524), (40308, 0.7141250732929876), (49083, 0.678510780285214), (41445, 0.6738304390613612), (44428, 0.6709360394959633), (66159, 0.6556073094298421), (50346, 0.6425003723243433), (76690, 0.6218229040324436), (47605, 0.6139214073516629), (44797, 0.571536067266591), (1282, 0.5349779633031715), (14118, 0.5103750079380108), (55166, 0.49592736841765284), (51226, 0.4909192653260308), (64491, 0.4840806142975823), (58792, 0.484033915515084), (47784, 0.43125369444671335), (35692, 0.4149874906543358), (7416, 0.38077117121015536), (2038, 0.34273111850868154), (63649, 0.28242042024265135), (57630, 0.27409050151757697), (68105, 0.23415386515749292), (77583, 0.23109353699195212), (48578, 0.21357429855330434), (31604, 0.2134513178468983), (76522, 0.207556614188947), (71865, 0.1862149450735977), (35407, 0.17818868976591473), (46826, 0.17562759085038204), (48338, 0.15157899384221674), (30195, 0.1399819649734777), (25180, 0.09314678973562962), (17959, 0.0677308798512638), (62442, 0.05225913113999159), (70991, 0.03773325457783672), (59774, 0.02242835758042172)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from recommenders.rating_boost import ScoreBoostRecommender\n",
    "\n",
    "bids = [\n",
    "    (idx, random.random()) for idx in random.sample(range(user_ratings.shape[1]), k=50)\n",
    "]\n",
    "paid_recommender = ScoreBoostRecommender(\n",
    "    predictor=latent,\n",
    "    users=user_ratings.shape[0],\n",
    "    items=user_ratings.shape[1],\n",
    "    bids=bids,\n",
    "    alpha=0.1,\n",
    "    beta=50,\n",
    "    promotion_slots=[True if x % 4 == 0 else False for x in range(20)]\n",
    ")\n",
    "print(\"Bids:\", sorted(bids, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02397b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without promotion: 158336 [87700, 231059, 203427, 189087, 227901, 215003, 227913, 218125, 257751, 209717, 188597, 189143, 257811, 270476, 188033, 148677, 170835, 211225, 172727, 267538]\n",
      "With promotion: 158336 [158242, 203427, 87700, 257751, 138446, 170835, 231059, 189087, 179985, 225455, 215003, 148677, 73868, 218125, 263479, 270476, 173121, 209717, 188597, 257871]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# print(recommender.users, recommender.items)\n",
    "# user = randint(0, user_ratings.shape[0])\n",
    "user = 158336\n",
    "print(\"Without promotion:\", user, [movie_id_mappings[x] for x in recommender.recommend_items(user, 20)])\n",
    "print(\"With promotion:\", user, [movie_id_mappings[x] for x in paid_recommender.recommend_items(user, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba902e",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
