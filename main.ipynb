{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0bc42d",
   "metadata": {},
   "source": [
    "# Demostration of Different Predictors and Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0b0f1",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0d34",
   "metadata": {},
   "source": [
    "### Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb430770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 5, 3, 3],\n",
    "    [3, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 5, 4],\n",
    "    [5, 1, 4, 2, 4],\n",
    "    [2, 3, 4, 1, 1],\n",
    "    [2, 3, 4, 2, 5],\n",
    "])\n",
    "\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (0, 3),\n",
    "    (1, 1),\n",
    "    (1, 4),\n",
    "    (2, 0),\n",
    "    (2, 4),\n",
    "    (3, 2),\n",
    "    (4, 1),\n",
    "    (4, 3),\n",
    "    (5, 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d794450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 4, 0, 5],\n",
    "    [0, 3, 5, 3, 4],\n",
    "    [5, 2, 0, 2, 3],\n",
    "    [0, 2, 3, 1, 2],\n",
    "    [4, 0, 5, 4, 5],\n",
    "    [5, 3, 0, 3, 5],\n",
    "    [3, 2, 3, 2, 0],\n",
    "    [5, 3, 4, 0, 5],\n",
    "    [4, 2, 5, 4, 0],\n",
    "    [5, 0, 5, 3, 4]\n",
    "])\n",
    "test_set = [(0, 4), (1, 3), (2, 3), (3, 1), (4, 2),\n",
    "                   (5, 0), (6, 1), (7, 1), (8, 0), (9, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8cd12",
   "metadata": {},
   "source": [
    "### Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samples import generate_sample_data\n",
    "import json\n",
    "\n",
    "sample_data = generate_sample_data(1000, 1000, 200)\n",
    "with open(\"samples/data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff1a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "with open(\"samples/data.json\", \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "user_ratings = sample_data[\"ratings\"]\n",
    "test_set_size = int(len(user_ratings) * len(user_ratings[0]) * 0.2)\n",
    "test_set = [(randrange(0, len(user_ratings)), randrange(0, len(user_ratings[0]))) for _ in range(test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eecd0c",
   "metadata": {},
   "source": [
    "### Real Datasets (from MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609255c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the latest (small) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")\n",
    "\n",
    "# Download the latest (full) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV files...\n",
      "Pivotting data...\n",
      "User ratings table created with dimensions: 330975 rows x 83239 columns\n",
      "Making test sets...\n",
      "Test set created with size: 6766432\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"Reading CSV files...\")\n",
    "# Read the ratings and movies CSV\n",
    "# ratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\")\n",
    "# movies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings and movies CSV (WARNING: FULL DATASET)\n",
    "ratings_df = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "movies_df = pd.read_csv(\"data/ml-latest/movies.csv\")\n",
    "\n",
    "# Convert the CSV into a user ratings table\n",
    "# Create a dense matrix where each row represents a user and each column a movie.\n",
    "# Missing ratings are filled with 0.\n",
    "print(\"Pivotting data...\")\n",
    "user_ids = sorted(ratings_df[\"userId\"].unique())\n",
    "movie_ids = sorted(ratings_df[\"movieId\"].unique())\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "rows = ratings_df[\"userId\"].map(user_id_map).values\n",
    "cols = ratings_df[\"movieId\"].map(movie_id_map).values\n",
    "data = ratings_df[\"rating\"].values\n",
    "\n",
    "user_ratings = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "movie_id_mappings = movies_df[\"movieId\"].to_list()\n",
    "\n",
    "print(\n",
    "    \"User ratings table created with dimensions:\",\n",
    "    user_ratings.shape[0],\n",
    "    \"rows x\",\n",
    "    user_ratings.shape[1],\n",
    "    \"columns\",\n",
    ")\n",
    "\n",
    "print(\"Making test sets...\")\n",
    "# Get all indices with an existing (non zero) rating\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = sample(valid_entries, min(test_set_size, len(valid_entries)))\n",
    "print(\"Test set created with size:\", len(test_set))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d69ca",
   "metadata": {},
   "source": [
    "## Rating Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3cb7",
   "metadata": {},
   "source": [
    "### Least Squares Optimiation Predictor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa88af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:29<00:00, 231213.38it/s]\n",
      "100%|██████████| 305862/305862 [00:06<00:00, 46284.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:38<00:00, 174396.53it/s]\n",
      "100%|██████████| 305862/305862 [00:08<00:00, 34573.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n",
      "Constructing relevant matrices...\n",
      "Calculating user and item biases...\n"
     ]
    }
   ],
   "source": [
    "from predictors.least_squares import LeastSquaresPredictor\n",
    "from utils import get_test_set_matrix, remove_test_set, root_mean_square_error, root_mean_square_error_entries\n",
    "\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)\n",
    "baseline = LeastSquaresPredictor(training_data=training_data, lmda=0.2)\n",
    "baseline.train()\n",
    "test_predictions = baseline.predict(test_set)\n",
    "all_predictions = baseline.predict_all()\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{all_predictions.data = }\")\n",
    "print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error(all_predictions, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f4b1",
   "metadata": {},
   "source": [
    "### Neighbor Correlations Predictor (based on Least Sqaures Optimization) (Improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af6f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cosine similarity coefficients...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# baseline.predict_all = lambda quiet=False: np.array(\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     [\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#         [np.nan, 2.7, 3.3, np.nan, 4.5],\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     16\u001b[39m improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mimproved\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwo_most_similar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m test_predictions = improved.predict(test_set)\n\u001b[32m     19\u001b[39m all_predictions = improved.predict_all()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/predictors/neighbor_correlations.py:191\u001b[39m, in \u001b[36mNeighborCorrelationsPredictor.train\u001b[39m\u001b[34m(self, get_neighbors)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, get_neighbors: Any = most_similar):\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating cosine similarity coefficients...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28mself\u001b[39m.error = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_less\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m - \u001b[38;5;28mself\u001b[39m.baseline.predict_all(quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m.cosine_coefficients = \u001b[38;5;28mself\u001b[39m._find_cosine_coefficients(\u001b[38;5;28mself\u001b[39m.error)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMaking neighbor table...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/numpy/ma/core.py:2083\u001b[39m, in \u001b[36mmasked_less\u001b[39m\u001b[34m(x, value, copy)\u001b[39m\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmasked_less\u001b[39m(x, value, copy=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   2060\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2061\u001b[39m \u001b[33;03m    Mask an array where less than a given value.\u001b[39;00m\n\u001b[32m   2062\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2081\u001b[39m \n\u001b[32m   2082\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m masked_where(\u001b[43mless\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m, x, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/numpy/ma/core.py:1065\u001b[39m, in \u001b[36m_MaskedBinaryOperation.__call__\u001b[39m\u001b[34m(self, a, b, *args, **kwargs)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate():\n\u001b[32m   1064\u001b[39m     np.seterr(divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# Get the mask for the result\u001b[39;00m\n\u001b[32m   1067\u001b[39m (ma, mb) = (getmask(a), getmask(b))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py:417\u001b[39m, in \u001b[36m_spbase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nnz != \u001b[32m0\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe truth value of an array with more than one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.neighbor_correlations import Correlation, NeighborCorrelationsPredictor\n",
    "from utils.neighbor_selection import most_similar, two_most_similar_skip_masked, two_most_similar\n",
    "\n",
    "# baseline.predict_all = lambda quiet=False: np.array(\n",
    "#     [\n",
    "#         [np.nan, 2.7, 3.3, np.nan, 4.5],\n",
    "#         [4.1, np.nan, 3.5, 4.9, np.nan],\n",
    "#         [np.nan, 3.8, 2.5, 4.2, np.nan],\n",
    "#         [2.8, 3.1, np.nan, 2.6, 4.8],\n",
    "#         [3.3, np.nan, 3.7, np.nan, 2.4],\n",
    "#         [np.nan, 3.9, 4.0, 1.5, 3.9],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.USER)\n",
    "improved.train(two_most_similar)\n",
    "test_predictions = improved.predict(test_set)\n",
    "all_predictions = improved.predict_all()\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{all_predictions = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad0bce",
   "metadata": {},
   "source": [
    "### Latent Factor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = [\n",
    "    [3, 4, 5, 3, 2, 3],\n",
    "    [3, 2, 3, 4, 2, 1],\n",
    "    [4, 4, 4, 5, 3, 2],\n",
    "    [3, 5, 4, 4, 3, 4],\n",
    "    [2, 1, 2, 2, 3, 1],\n",
    "    [3, 5, 5, 4, 4, 3],\n",
    "    [3, 5, 5, 3, 2, 2],\n",
    "    [2, 3, 3, 2, 1, 2],\n",
    "]\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 0),\n",
    "    (3, 1),\n",
    "    (5, 1),\n",
    "    (5, 4),\n",
    "    (6, 0),\n",
    "    (6, 2),\n",
    "    (7, 1),\n",
    "    (7, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd8b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:07<00:00, 856388.87it/s] \n",
      "100%|██████████| 305867/305867 [00:06<00:00, 46375.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:08<00:00, 787318.25it/s] \n",
      "100%|██████████| 305867/305867 [00:08<00:00, 37817.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n",
      "latent.p = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 330975))\n",
      "latent.q = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(2, 83239))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.latent_factor import LatentFactorPredictor\n",
    "from utils import get_test_set_matrix, remove_test_set\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)\n",
    "u, i = training_data.shape\n",
    "k = 2\n",
    "latent = LatentFactorPredictor(\n",
    "    training_data=training_data,\n",
    "    k=k,\n",
    "    p=np.ones(shape=(k,u), dtype=np.float64),\n",
    "    q=np.ones(shape=(k,i), dtype=np.float64),\n",
    "    lmda=0.2,\n",
    ")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b832b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Performing alternating least squares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:20<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent.train(100)\n",
    "# t += 20\n",
    "# print(f\"Total: {t} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d91660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6766432/6766432 [00:43<00:00, 154736.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([3.58489921, 4.18723421, 3.87480545, ..., 4.67600189, 1.50073983,\n",
      "       2.93470728], shape=(6766432,))\n",
      "latent.p = array([[1.0491592 , 0.97073347, 1.21726924, ..., 0.90782191, 0.88881196,\n",
      "        0.62112248],\n",
      "       [0.65405623, 0.92501878, 1.01531951, ..., 0.87314036, 1.03018454,\n",
      "        0.9212989 ]], shape=(2, 330975))\n",
      "latent.q = array([[ 3.60450119,  4.55718967,  4.60851564, ...,  1.6804029 ,\n",
      "         1.54695769,  1.        ],\n",
      "       [ 0.46085957, -1.21818368, -1.49743794, ...,  1.84620913,\n",
      "         1.33473082,  1.        ]], shape=(2, 83239))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# all_predictions = latent.predict_all()\n",
    "test_predictions = latent.predict(test_set)\n",
    "# print(f\"{all_predictions = }\")\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9695964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test = np.float64(0.8370787523136615)\n"
     ]
    }
   ],
   "source": [
    "from utils import root_mean_square_error_entries, root_mean_square_error\n",
    "\n",
    "# rmse_training = root_mean_square_error(all_predictions, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "# print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f670e",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a49a7",
   "metadata": {},
   "source": [
    "### Plain Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1445f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.plain import PlainRecommender\n",
    "\n",
    "recommender = PlainRecommender(\n",
    "    predictor=latent, users=user_ratings.shape[0], items=user_ratings.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc4f74",
   "metadata": {},
   "source": [
    "### Pure Score Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a9260f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bids: [(35578, 0.9857356122403614), (24940, 0.9836144124647167), (66678, 0.9565546327049057), (66980, 0.912111671217647), (45457, 0.9109811666088713), (61321, 0.9066169493684465), (13989, 0.874543573549682), (10456, 0.849657074742543), (29338, 0.8305855852107319), (4331, 0.7455251112075354), (80947, 0.7098419354265786), (49215, 0.7008575274819981), (75060, 0.6799938029094219), (36085, 0.6738949284639714), (32880, 0.6550419867271192), (45070, 0.654454699965262), (69238, 0.6331872130915799), (10159, 0.6159643584504674), (8027, 0.6057263002464343), (80124, 0.5888029371139819), (19807, 0.5665873627163729), (19786, 0.5621361453866365), (33317, 0.5488157070109523), (1082, 0.5336509278215987), (15585, 0.5307869398675993), (25761, 0.5091825033199299), (81222, 0.49727522358567844), (5469, 0.4799005630039437), (15637, 0.4437888806072867), (77804, 0.4308181462664381), (77318, 0.42239667167058803), (52673, 0.40805810448361746), (79821, 0.36670815096569376), (70801, 0.35681284956365134), (9629, 0.35451799423075947), (21223, 0.3498941411875597), (44393, 0.30720027986076515), (7588, 0.23271619989190107), (54146, 0.21616784302076997), (9132, 0.21483410244964374), (62425, 0.183782744918347), (31226, 0.14743735520952062), (28964, 0.1290819094260136), (9573, 0.09472468149885893), (18405, 0.08758473136725609), (24168, 0.07670609906435055), (67979, 0.07329856288827319), (65155, 0.04252488821913103), (82300, 0.03762156262358718), (6594, 0.03069951463292364)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from recommenders.score_boost import ScoreBoostRecommender\n",
    "\n",
    "bids = [\n",
    "    (idx, random.random()) for idx in random.sample(range(user_ratings.shape[1]), k=50)\n",
    "]\n",
    "paid_recommender = ScoreBoostRecommender(\n",
    "    predictor=latent,\n",
    "    users=user_ratings.shape[0],\n",
    "    items=user_ratings.shape[1],\n",
    "    bids=bids,\n",
    "    alpha=0.1,\n",
    "    beta=50,\n",
    "    promotion_slots=[True if x % 4 == 0 else False for x in range(20)]\n",
    ")\n",
    "print(\"Bids:\", sorted(bids, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02397b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "Without promotion: 21251 [np.int64(73246), np.int64(36634), np.int64(64467), np.int64(63232), np.int64(54986), np.int64(61627), np.int64(66531), np.int64(79181), np.int64(81303), np.int64(33769), np.int64(50025), np.int64(79845), np.int64(77454), np.int64(42340), np.int64(70074), np.int64(62918), np.int64(54266), np.int64(36404), np.int64(50038), np.int64(36109)]\n",
      "With promotion: 21251 [np.int64(19807), np.int64(80868), np.int64(57313), np.int64(70867), np.int64(44393), np.int64(73246), np.int64(44646), np.int64(28092), np.int64(9629), np.int64(57450), np.int64(44596), np.int64(70946), np.int64(66980), np.int64(64743), np.int64(36109), np.int64(49520), np.int64(24168), np.int64(73161), np.int64(74935), np.int64(53074)]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# print(recommender.users, recommender.items)\n",
    "user = randint(0, user_ratings.shape[0])\n",
    "# user = 261\n",
    "print(\"Without promotion:\", user, [x for x in recommender.recommend_items(user, 20)])\n",
    "print(\"With promotion:\", user, [x for x in paid_recommender.recommend_items(user, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba902e",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
