{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0bc42d",
   "metadata": {},
   "source": [
    "# Demostration of Different Predictors and Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0b0f1",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0d34",
   "metadata": {},
   "source": [
    "### Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb430770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "user_ratings = csr_matrix([\n",
    "    [5, 4, 5, 3, 3],\n",
    "    [3, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 5, 4],\n",
    "    [5, 1, 4, 2, 4],\n",
    "    [2, 3, 4, 1, 1],\n",
    "    [2, 3, 4, 2, 5],\n",
    "])\n",
    "\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (0, 3),\n",
    "    (1, 1),\n",
    "    (1, 4),\n",
    "    (2, 0),\n",
    "    (2, 4),\n",
    "    (3, 2),\n",
    "    (4, 1),\n",
    "    (4, 3),\n",
    "    (5, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d794450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [5, 4, 4, 0, 5],\n",
    "        [0, 3, 5, 3, 4],\n",
    "        [5, 2, 0, 2, 3],\n",
    "        [0, 2, 3, 1, 2],\n",
    "        [4, 0, 5, 4, 5],\n",
    "        [5, 3, 0, 3, 5],\n",
    "        [3, 2, 3, 2, 0],\n",
    "        [5, 3, 4, 0, 5],\n",
    "        [4, 2, 5, 4, 0],\n",
    "        [5, 0, 5, 3, 4],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 4),\n",
    "    (1, 3),\n",
    "    (2, 3),\n",
    "    (3, 1),\n",
    "    (4, 2),\n",
    "    (5, 0),\n",
    "    (6, 1),\n",
    "    (7, 1),\n",
    "    (8, 0),\n",
    "    (9, 0),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8cd12",
   "metadata": {},
   "source": [
    "### Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from samples import generate_sample_data\n",
    "\n",
    "sample_data = generate_sample_data(100000, 5000, 20)\n",
    "with open(\"samples/data\", \"wb\") as f:\n",
    "    pickle.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff1a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import randrange, sample\n",
    "with open(\"samples/data\", \"rb\") as f:\n",
    "    sample_data = pickle.load(f)\n",
    "\n",
    "user_ratings = sample_data[\"ratings\"]\n",
    "test_set_size = int(user_ratings.shape[0] * user_ratings.shape[1] * 0.2)\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "\n",
    "shuffuled_valid_entries = sample(valid_entries, k=len(valid_entries))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = shuffuled_valid_entries[:test_set_size]\n",
    "training_set = shuffuled_valid_entries[test_set_size:]\n",
    "# test_set = [(randrange(0, len(user_ratings)), randrange(0, len(user_ratings[0]))) for _ in range(test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eecd0c",
   "metadata": {},
   "source": [
    "### Real Datasets (from MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609255c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the latest (small) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")\n",
    "\n",
    "# Download the latest (full) dataset\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n",
    "response = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Unzip the dataset into a folder\n",
    "z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09abda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV files...\n",
      "Pivotting data...\n",
      "User ratings table created with dimensions: 610 rows x 9724 columns\n",
      "Making test sets...\n",
      "Test set created with size: 20167\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"Reading CSV files...\")\n",
    "# Read the ratings and movies CSV\n",
    "ratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\")\n",
    "movies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Read the ratings and movies CSV (WARNING: FULL DATASET)\n",
    "# ratings_df = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "# movies_df = pd.read_csv(\"data/ml-latest/movies.csv\")\n",
    "\n",
    "# Convert the CSV into a user ratings table\n",
    "# Create a dense matrix where each row represents a user and each column a movie.\n",
    "# Missing ratings are filled with 0.\n",
    "print(\"Pivotting data...\")\n",
    "user_ids = sorted(ratings_df[\"userId\"].unique())\n",
    "movie_ids = sorted(ratings_df[\"movieId\"].unique())\n",
    "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "movie_id_map = {mid: j for j, mid in enumerate(movie_ids)}\n",
    "\n",
    "rows = ratings_df[\"userId\"].map(user_id_map).values\n",
    "cols = ratings_df[\"movieId\"].map(movie_id_map).values\n",
    "data = ratings_df[\"rating\"].values\n",
    "\n",
    "user_ratings = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "movie_id_mappings = movies_df[\"movieId\"].to_list()\n",
    "\n",
    "print(\n",
    "    \"User ratings table created with dimensions:\",\n",
    "    user_ratings.shape[0],\n",
    "    \"rows x\",\n",
    "    user_ratings.shape[1],\n",
    "    \"columns\",\n",
    ")\n",
    "\n",
    "print(\"Making test sets...\")\n",
    "# Get all indices with an existing (non zero) rating\n",
    "valid_entries = list(zip(*user_ratings.nonzero()))\n",
    "shuffuled_valid_entries = sample(valid_entries, k=len(valid_entries))\n",
    "test_set_size = int(len(valid_entries) * 0.2)\n",
    "\n",
    "# Randomly select test_set_size indices from the valid entries\n",
    "test_set = shuffuled_valid_entries[:test_set_size]\n",
    "training_set = shuffuled_valid_entries[test_set_size:]\n",
    "print(\"Test set created with size:\", len(test_set))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22a54b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38cbf519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test set entries from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:02<00:00, 896399.90it/s] \n",
      "100%|██████████| 100000/100000 [00:02<00:00, 43343.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing test set entries.\n",
      "Getting test set matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:01<00:00, 1046648.60it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 35032.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting test set matrix.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_test_set_matrix, remove_test_set\n",
    "\n",
    "training_data = remove_test_set(user_ratings, test_set)\n",
    "test_data = get_test_set_matrix(user_ratings, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d69ca",
   "metadata": {},
   "source": [
    "## Rating Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d3cb7",
   "metadata": {},
   "source": [
    "### Least Squares Optimiation Predictor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa88af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing relevant matrices for iterative training...\n",
      "Calculating user and item biases using Conjugate Gradient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/10000 [00:00<03:37, 45.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:08<00:00, 238452.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [00:31<00:00, 250849.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [00:25<00:00, 312365.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:05<00:00, 352209.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.3365181452088841)\n",
      "rmse_test = np.float64(0.34137219775811245)\n"
     ]
    }
   ],
   "source": [
    "from predictors.least_squares import LeastSquaresPredictor\n",
    "from utils import root_mean_square_error_entries\n",
    "\n",
    "\n",
    "baseline = LeastSquaresPredictor(shape=user_ratings.shape, lmda=0.2)\n",
    "baseline.iterative_train(training_data=training_data)\n",
    "test_predictions = baseline.predict(test_set)\n",
    "training_predictions = baseline.predict(training_set)\n",
    "# print(f\"{test_predictions = }\")\n",
    "# print(f\"{training_predictions = }\")\n",
    "# print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef31880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing relevant matrices...\n",
      "Calculating user and item biases...\n",
      "Training done.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 108660.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 275337.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = [np.float64(4.6178451178451185), np.float64(3.486111111111111), np.float64(2.7811447811447816), np.float64(1.0), np.float64(4.789983164983165), np.float64(4.877104377104377), np.float64(1.2289562289562284), np.float64(2.9191919191919187), np.float64(4.784511784511785), np.float64(4.609848484848484)]\n",
      "training_predictions = [np.float64(5.0), np.float64(3.093855218855219), np.float64(4.895622895622896), np.float64(2.891414141414141), np.float64(4.693181818181818), np.float64(4.415404040404041), np.float64(4.103114478114478), np.float64(2.1864478114478114), np.float64(3.7104377104377106), np.float64(2.4949494949494944), np.float64(1.2878787878787876), np.float64(2.2171717171717167), np.float64(4.904882154882155), np.float64(3.582912457912458), np.float64(4.5122053872053876), np.float64(2.96043771043771), np.float64(3.5551346801346804), np.float64(4.48442760942761), np.float64(3.1456228956228953), np.float64(3.0307239057239053), np.float64(1.8236531986531985), np.float64(4.835858585858586), np.float64(4.720959595959596), np.float64(4.443181818181818), np.float64(2.8678451178451176), np.float64(4.669612794612795), np.float64(3.462542087542088), np.float64(4.494949494949495), np.float64(3.2878787878787876), np.float64(4.217171717171717)]\n",
      "training_data.data = array([5, 4, 4, 3, 5, 4, 5, 2, 3, 3, 1, 2, 4, 4, 5, 3, 3, 5, 3, 3, 2, 5,\n",
      "       4, 5, 2, 5, 4, 5, 3, 4])\n",
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 236077.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 156503.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.5126176872466319)\n",
      "rmse_test = np.float64(0.5848469310964864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline.train(training_data=training_data)\n",
    "test_predictions = baseline.predict(test_set)\n",
    "training_predictions = baseline.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{training_data.data = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafca403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/baseline', 'wb') as f:\n",
    "    pickle.dump(baseline, f)\n",
    "with open('data/training_set', 'wb') as f:\n",
    "    pickle.dump(training_set, f)\n",
    "with open('data/training_predictions', 'wb') as f:\n",
    "    pickle.dump(training_predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f4b1",
   "metadata": {},
   "source": [
    "### Neighbor Correlations Predictor (based on Least Sqaures Optimization) (Improved)\n",
    "As this requires calculating cosine coefficient for every single pair of items (or users, depending on the correlation chosen), this is extremely computationally expensive. Not recommended for data with a large number of items (or users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b09baf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/training_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodels/baseline\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     baseline = pickle.load(f)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/training_set\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     training_set = pickle.load(f)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdata/baseline_predictions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ESTR3302-project/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/training_set'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/baseline', 'rb') as f:\n",
    "    baseline = pickle.load(f)\n",
    "with open('data/training_set', 'rb') as f:\n",
    "    training_set = pickle.load(f)\n",
    "with open('data/baseline_predictions', 'rb') as f:\n",
    "    training_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af6f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix from baseline predictions...\n",
      "Calculating cosine similarity coefficients...\n",
      "Calculating numerators...\n",
      "Calculating denominators...\n",
      "Making neighbor table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:45<00:00, 350.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [06:20<00:00, 5250.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [24:18<00:00, 5484.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "test_predictions = array([2.53018404, 2.6618057 , 2.65310744, ..., 2.59309105, 2.59112454,\n",
      "       2.56499384], shape=(2000000,))\n",
      "training_predictions = array([2.60047641, 2.55844541, 2.67766904, ..., 2.61555986, 2.55858723,\n",
      "       2.57491603], shape=(8000000,))\n"
     ]
    }
   ],
   "source": [
    "from predictors.neighbor_correlations import Correlation, NeighborCorrelationsPredictor\n",
    "from utils.neighbor_selection import most_similar, two_most_similar_skip_masked, two_most_similar\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# baseline.predict_all = lambda quiet=False: np.array(\n",
    "#     [\n",
    "#         [np.nan, 2.7, 3.3, np.nan, 4.5],\n",
    "#         [4.1, np.nan, 3.5, 4.9, np.nan],\n",
    "#         [np.nan, 3.8, 2.5, 4.2, np.nan],\n",
    "#         [2.8, 3.1, np.nan, 2.6, 4.8],\n",
    "#         [3.3, np.nan, 3.7, np.nan, 2.4],\n",
    "#         [np.nan, 3.9, 4.0, 1.5, 3.9],\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "improved = NeighborCorrelationsPredictor(baseline=baseline, correlation=Correlation.ITEM)\n",
    "print(\"Creating sparse matrix from baseline predictions...\")\n",
    "rows, cols = zip(*training_set)\n",
    "baseline_prediction = csr_matrix((np.array(training_predictions), (rows, cols)), shape=baseline.shape)\n",
    "improved.train(training_data, two_most_similar_skip_masked, baseline_predictions=baseline_prediction)\n",
    "test_predictions = improved.predict(test_set)\n",
    "training_predictions = improved.predict(training_set)\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{training_predictions = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24aa72df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [00:25<00:00, 309402.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:05<00:00, 334018.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.3340520910273014)\n",
      "rmse_test = np.float64(0.3437479297468506)\n"
     ]
    }
   ],
   "source": [
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_training = }\")\n",
    "print(f\"{rmse_test = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad0bce",
   "metadata": {},
   "source": [
    "### Latent Factor Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_ratings = csr_matrix(\n",
    "    [\n",
    "        [3, 4, 5, 3, 2, 3],\n",
    "        [3, 2, 3, 4, 2, 1],\n",
    "        [4, 4, 4, 5, 3, 2],\n",
    "        [3, 5, 4, 4, 3, 4],\n",
    "        [2, 1, 2, 2, 3, 1],\n",
    "        [3, 5, 5, 4, 4, 3],\n",
    "        [3, 5, 5, 3, 2, 2],\n",
    "        [2, 3, 3, 2, 1, 2],\n",
    "    ]\n",
    ")\n",
    "test_set = [\n",
    "    (0, 0),\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 0),\n",
    "    (3, 1),\n",
    "    (5, 1),\n",
    "    (5, 4),\n",
    "    (6, 0),\n",
    "    (6, 2),\n",
    "    (7, 1),\n",
    "    (7, 3),\n",
    "]\n",
    "training_set = []\n",
    "for i in range(user_ratings.shape[0]):\n",
    "    for j in range(user_ratings.shape[1]):\n",
    "        if (i, j) not in test_set and user_ratings[i, j] != 0:\n",
    "            training_set.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd8b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent.p = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(5, 100000))\n",
      "latent.q = array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], shape=(5, 5000))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from predictors.latent_factor import LatentFactorPredictor\n",
    "\n",
    "u, i = training_data.shape\n",
    "k = 5\n",
    "latent = LatentFactorPredictor(\n",
    "    shape=training_data.shape,\n",
    "    k=k,\n",
    "    p=np.ones(shape=(k,u), dtype=np.float64),\n",
    "    q=np.ones(shape=(k,i), dtype=np.float64),\n",
    "    lmda=0.2,\n",
    ")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b832b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Performing alternating least squares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8660/10000 [3:01:44<28:07,  1.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent.train(training_data=training_data, max_iterations=10000, tol=1e-4)\n",
    "# t += 20\n",
    "# print(f\"Total: {t} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d91660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [00:46<00:00, 173505.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "Predicting entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:10<00:00, 184370.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting entries.\n",
      "training_predictions = array([2.67166896, 2.62244164, 2.65717642, ..., 2.45798514, 2.65019089,\n",
      "       2.4884156 ], shape=(8000000,))\n",
      "test_predictions = array([2.45227672, 2.91494366, 2.48036351, ..., 2.61548735, 2.60261286,\n",
      "       2.55135824], shape=(2000000,))\n",
      "latent.p = array([[0.57688248, 0.15458079, 0.36187863, ..., 0.38679631, 0.46600835,\n",
      "        0.24079034],\n",
      "       [0.0499925 , 0.28714986, 0.44058903, ..., 0.14953331, 0.35944786,\n",
      "        0.35633317],\n",
      "       [0.36054538, 0.45050216, 0.16488546, ..., 0.44992167, 0.28915843,\n",
      "        0.25110507],\n",
      "       [0.53410211, 0.16454649, 0.39368848, ..., 0.40303429, 0.29763438,\n",
      "        0.47025231],\n",
      "       [0.21567791, 0.65270152, 0.38527362, ..., 0.31313011, 0.31733939,\n",
      "        0.38812071]], shape=(5, 100000))\n",
      "latent.q = array([[1.07538268, 1.07047689, 2.34703052, ..., 0.86255349, 1.07254012,\n",
      "        1.81381774],\n",
      "       [1.07523625, 1.13362167, 0.72532287, ..., 2.68978183, 1.99626121,\n",
      "        1.63366244],\n",
      "       [1.31197831, 2.5948222 , 1.94031426, ..., 0.32082199, 1.34810685,\n",
      "        1.24291351],\n",
      "       [2.27989081, 1.72096913, 0.88558425, ..., 2.02689594, 1.30249437,\n",
      "        1.07157663],\n",
      "       [2.01151375, 1.06849875, 1.67608204, ..., 1.7310692 , 2.07964635,\n",
      "        1.87957274]], shape=(5, 5000))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_predictions = latent.predict(training_set)\n",
    "test_predictions = latent.predict(test_set)\n",
    "print(f\"{training_predictions = }\")\n",
    "print(f\"{test_predictions = }\")\n",
    "print(f\"{latent.p = }\")\n",
    "print(f\"{latent.q = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9695964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000000/2000000 [00:04<00:00, 415989.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_test = np.float64(0.3235141820151544)\n",
      "Gathering entries from predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000000/8000000 [00:20<00:00, 386326.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_training = np.float64(0.30021525799094384)\n"
     ]
    }
   ],
   "source": [
    "from utils import root_mean_square_error_entries, root_mean_square_error\n",
    "\n",
    "rmse_test = root_mean_square_error_entries(test_predictions, test_set, test_data)\n",
    "print(f\"{rmse_test = }\")\n",
    "rmse_training = root_mean_square_error_entries(training_predictions, training_set, training_data)\n",
    "print(f\"{rmse_training = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e5ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/latent_' + str(k), 'wb') as f:\n",
    "    pickle.dump(latent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f670e",
   "metadata": {},
   "source": [
    "## Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd1b2f",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03c5f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/latent_5', 'rb') as f:\n",
    "    latent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a49a7",
   "metadata": {},
   "source": [
    "### Plain Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1445f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.plain import PlainRecommender\n",
    "\n",
    "recommender = PlainRecommender(\n",
    "    predictor=latent, users=user_ratings.shape[0], items=user_ratings.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc4f74",
   "metadata": {},
   "source": [
    "### Pure Score Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9260f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ScoreBoostRecommender' from 'recommenders.rating_boost' (/Users/matthewngan/ESTR3302-project/recommenders/rating_boost.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrecommenders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrating_boost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ScoreBoostRecommender\n\u001b[32m      4\u001b[39m bids = [\n\u001b[32m      5\u001b[39m     (idx, random.random()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m random.sample(\u001b[38;5;28mrange\u001b[39m(user_ratings.shape[\u001b[32m1\u001b[39m]), k=\u001b[32m50\u001b[39m)\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      7\u001b[39m paid_recommender = ScoreBoostRecommender(\n\u001b[32m      8\u001b[39m     predictor=latent,\n\u001b[32m      9\u001b[39m     users=user_ratings.shape[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     promotion_slots=[\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x % \u001b[32m4\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m)]\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ScoreBoostRecommender' from 'recommenders.rating_boost' (/Users/matthewngan/ESTR3302-project/recommenders/rating_boost.py)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from recommenders.rating_boost import ScoreBoostRecommender\n",
    "\n",
    "bids = [\n",
    "    (idx, random.random()) for idx in random.sample(range(user_ratings.shape[1]), k=50)\n",
    "]\n",
    "paid_recommender = ScoreBoostRecommender(\n",
    "    predictor=latent,\n",
    "    users=user_ratings.shape[0],\n",
    "    items=user_ratings.shape[1],\n",
    "    bids=bids,\n",
    "    alpha=0.1,\n",
    "    beta=50,\n",
    "    promotion_slots=[True if x % 4 == 0 else False for x in range(20)]\n",
    ")\n",
    "print(\"Bids:\", sorted(bids, reverse=True, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02397b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without promotion: 158336 [87700, 231059, 203427, 189087, 227901, 215003, 227913, 218125, 257751, 209717, 188597, 189143, 257811, 270476, 188033, 148677, 170835, 211225, 172727, 267538]\n",
      "With promotion: 158336 [158242, 203427, 87700, 257751, 138446, 170835, 231059, 189087, 179985, 225455, 215003, 148677, 73868, 218125, 263479, 270476, 173121, 209717, 188597, 257871]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# print(recommender.users, recommender.items)\n",
    "# user = randint(0, user_ratings.shape[0])\n",
    "user = 158336\n",
    "print(\"Without promotion:\", user, [movie_id_mappings[x] for x in recommender.recommend_items(user, 20)])\n",
    "print(\"With promotion:\", user, [movie_id_mappings[x] for x in paid_recommender.recommend_items(user, 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd334dd",
   "metadata": {},
   "source": [
    "### Accuracies\n",
    "\n",
    "Baseline lambda = 0.2, rmse test = 0.8620847479490242, rmse train = 0.8476892585276561\n",
    "Latent 2, rmse test = 0.8363286658663873, rmse train = 0.8070657315515645\n",
    "Latent 5, rmse test = 0.8169738957584134, rmse train = 0.7484386486325613\n",
    "Latent 8, rmse test = 0.8173996547778605, rmse train = 0.7102392483099188\n",
    "Latent 10, rmse test = 0.8216205577284467, rmse train = 0.6902388489455976 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba902e",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
